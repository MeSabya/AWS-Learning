![image](https://user-images.githubusercontent.com/33947539/155696360-97b7cf47-ed2e-450c-a243-62c26e0c4d80.png)

**Explanation**: 

To manage your objects so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:

Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them or archive objects to the S3 Glacier storage class one year after creating them.
There are costs associated with the lifecycle transition requests.

Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf.
The lifecycle expiration costs depend on when you choose to expire objects. The solutions architect can create a lifecycle action using the “expiration action element”, which expires objects (deletes them) at the specified time.

INCORRECT: “Store the files on Amazon EBS, and create a lifecycle policy to remove the files after three months.” is incorrect. There is no lifecycle policy available for deleting files on EBS. The Amazon Data Lifecycle Manager (DLM) feature automates the creation, retention, and deletion of EBS snapshots but not the individual files within an EBS volume.

INCORRECT: “Store the files on Amazon Glacier, and create a lifecycle policy to remove the files after three months.” is incorrect. S3 lifecycle actions apply to any storage class, including Glacier; however, Glacier would not allow for an immediate download.

CORRECT: “Store the files on Amazon S3, and create a lifecycle policy to remove the files after three months.” is the correct answer.

INCORRECT: “Store the files on Amazon EFS, and create a lifecycle policy to remove the files after three months.” is incorrect. There is no lifecycle policy available for deleting files on EFS

![image](https://user-images.githubusercontent.com/33947539/155693047-5340f20d-4d05-4050-8f64-f619b2222a92.png)

**Explanation:**
Amazon DynamoDB global tables provide a fully managed solution for deploying a multiregion, multi-master database without having to build and maintain your own replication solution. With global tables, you can specify the AWS Regions where you want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these Regions and propagate ongoing data changes to all of them.

![image](https://user-images.githubusercontent.com/33947539/155694700-a48de9d0-dcb3-4d4e-ac02-f8333a65583a.png)

DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. In such an environment, users expect very fast application performance. Global tables provide automatic multi-master replication to AWS Regions worldwide. They enable you to deliver low-latency data access to your users no matter where they are located.

CORRECT: “DynamoDB with Global Tables and Multi-Region Replication” is the correct answer.

INCORRECT: “EC2 instances with EBS replication” is incorrect. There is no such thing as EBS replication. You could build your own database stack on EC2 with DB-level replication, but that is not what is stated in the answer.

INCORRECT: “S3 with Cross Region Replication” is incorrect. S3 is an object store, not a multi-master database.

INCORRECT: “RDS with Multi-AZ” is incorrect. RDS with Multi-AZ is not multi-master (only one DB can be written to at a time) and does not span regions.

![image](https://user-images.githubusercontent.com/33947539/155695856-6024c67f-21a6-47e0-aead-7cba164227d8.png)

**Explanation:**
ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. The in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads or compute-intensive workloads

There are two different database engines with different characteristics as shown below:

![image](https://user-images.githubusercontent.com/33947539/155695970-824719ef-04d6-4a25-a36d-dd4f8545e680.png)

The correct choice for this scenario is Redis because it provides the required persistence.

INCORRECT: “RDS in a multi-AZ configuration” is incorrect. RDS is not the optimum solution due to the requirement to optimize retrieval times. Retrieval times are faster with an in-memory data store such as ElastiCache.

CORRECT: “ElastiCache with the Redis engine” is the correct answer.

INCORRECT: “Kinesis Data Streams” is incorrect. Kinesis Data Streams is used for processing streams of data; it is not a persistent data store.

INCORRECT: “ElastiCache with the Memcached engine” is incorrect as Memcached does not offer persistence.

![image](https://user-images.githubusercontent.com/33947539/155696755-83f8d3b6-afe6-44c8-90ae-a4b4ef4c4a23.png)

**Explanation:**

Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public key cryptography uses a public key to encrypt a piece of data, and then the recipient uses the private key to decrypt the data. The public keys (stored by AWS) and private keys (stored by the recipient) are known as a key pair. Public key cryptography enables you to securely access your instances using a private key instead of a password.

For Windows AMIs, the private key file is required to obtain the password used to log into your instance.
For Linux AMIs, the private key file allows you to securely SSH into your instance.
INCORRECT: “SSL/TLS certificate” is incorrect as you cannot securely access an instance to run commands using an SSL/TLS certificate.

INCORRECT: “Public key” is incorrect. You cannot login to an EC2 instance using certificates/public keys.

CORRECT: “Key pairs” is the correct answer.

INCORRECT: “EC2 password” is incorrect. The “EC2 password” might refer to the operating system password. By default, you cannot log in this way to Linux and must instead use a key pair. However, this can be enabled by setting a password and updating the /etc/ssh/sshd_config file.
