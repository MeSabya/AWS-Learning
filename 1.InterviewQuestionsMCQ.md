![image](https://user-images.githubusercontent.com/33947539/155696360-97b7cf47-ed2e-450c-a243-62c26e0c4d80.png)

**Explanation**: 

To manage your objects so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:

Transition actions — Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them or archive objects to the S3 Glacier storage class one year after creating them.
There are costs associated with the lifecycle transition requests.

Expiration actions — Define when objects expire. Amazon S3 deletes expired objects on your behalf.
The lifecycle expiration costs depend on when you choose to expire objects. The solutions architect can create a lifecycle action using the “expiration action element”, which expires objects (deletes them) at the specified time.

INCORRECT: “Store the files on Amazon EBS, and create a lifecycle policy to remove the files after three months.” is incorrect. There is no lifecycle policy available for deleting files on EBS. The Amazon Data Lifecycle Manager (DLM) feature automates the creation, retention, and deletion of EBS snapshots but not the individual files within an EBS volume.

INCORRECT: “Store the files on Amazon Glacier, and create a lifecycle policy to remove the files after three months.” is incorrect. S3 lifecycle actions apply to any storage class, including Glacier; however, Glacier would not allow for an immediate download.

CORRECT: “Store the files on Amazon S3, and create a lifecycle policy to remove the files after three months.” is the correct answer.

INCORRECT: “Store the files on Amazon EFS, and create a lifecycle policy to remove the files after three months.” is incorrect. There is no lifecycle policy available for deleting files on EFS

![image](https://user-images.githubusercontent.com/33947539/155693047-5340f20d-4d05-4050-8f64-f619b2222a92.png)

**Explanation:**
Amazon DynamoDB global tables provide a fully managed solution for deploying a multiregion, multi-master database without having to build and maintain your own replication solution. With global tables, you can specify the AWS Regions where you want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these Regions and propagate ongoing data changes to all of them.

![image](https://user-images.githubusercontent.com/33947539/155694700-a48de9d0-dcb3-4d4e-ac02-f8333a65583a.png)

DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. In such an environment, users expect very fast application performance. Global tables provide automatic multi-master replication to AWS Regions worldwide. They enable you to deliver low-latency data access to your users no matter where they are located.

CORRECT: “DynamoDB with Global Tables and Multi-Region Replication” is the correct answer.

INCORRECT: “EC2 instances with EBS replication” is incorrect. There is no such thing as EBS replication. You could build your own database stack on EC2 with DB-level replication, but that is not what is stated in the answer.

INCORRECT: “S3 with Cross Region Replication” is incorrect. S3 is an object store, not a multi-master database.

INCORRECT: “RDS with Multi-AZ” is incorrect. RDS with Multi-AZ is not multi-master (only one DB can be written to at a time) and does not span regions.

![image](https://user-images.githubusercontent.com/33947539/155695856-6024c67f-21a6-47e0-aead-7cba164227d8.png)

**Explanation:**
ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. The in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads or compute-intensive workloads

There are two different database engines with different characteristics as shown below:

![image](https://user-images.githubusercontent.com/33947539/155695970-824719ef-04d6-4a25-a36d-dd4f8545e680.png)

The correct choice for this scenario is Redis because it provides the required persistence.

INCORRECT: “RDS in a multi-AZ configuration” is incorrect. RDS is not the optimum solution due to the requirement to optimize retrieval times. Retrieval times are faster with an in-memory data store such as ElastiCache.

CORRECT: “ElastiCache with the Redis engine” is the correct answer.

INCORRECT: “Kinesis Data Streams” is incorrect. Kinesis Data Streams is used for processing streams of data; it is not a persistent data store.

INCORRECT: “ElastiCache with the Memcached engine” is incorrect as Memcached does not offer persistence.

![image](https://user-images.githubusercontent.com/33947539/155696755-83f8d3b6-afe6-44c8-90ae-a4b4ef4c4a23.png)

**Explanation:**

Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public key cryptography uses a public key to encrypt a piece of data, and then the recipient uses the private key to decrypt the data. The public keys (stored by AWS) and private keys (stored by the recipient) are known as a key pair. Public key cryptography enables you to securely access your instances using a private key instead of a password.

For Windows AMIs, the private key file is required to obtain the password used to log into your instance.
For Linux AMIs, the private key file allows you to securely SSH into your instance.
INCORRECT: “SSL/TLS certificate” is incorrect as you cannot securely access an instance to run commands using an SSL/TLS certificate.

INCORRECT: “Public key” is incorrect. You cannot login to an EC2 instance using certificates/public keys.

CORRECT: “Key pairs” is the correct answer.

INCORRECT: “EC2 password” is incorrect. The “EC2 password” might refer to the operating system password. By default, you cannot log in this way to Linux and must instead use a key pair. However, this can be enabled by setting a password and updating the /etc/ssh/sshd_config file.

![image](https://user-images.githubusercontent.com/33947539/155697012-59af233c-5d9e-4406-aa50-bd9d7c5785e4.png)

**Explanation:**

All objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.

When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and the expiration date and time. The pre-signed URLs are valid only for the specified duration.

Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.

INCORRECT: “Enable public read access for the S3 bucket.” is incorrect. Enabling public read access does not restrict the content to authorized consumers.

INCORRECT: “Use CloudFront to distribute the files using authorization hashtags.” is incorrect. You cannot use CloudFront as hashtags are not a CloudFront authentication mechanism.

CORRECT: “Generate a pre-signed URL and distribute it to the consumers.” is the correct answer.

INCORRECT: “Configure an allow rule in the Security Group for the IP addresses of the consumers.” is incorrect. Security Groups do not apply to S3 buckets.

![image](https://user-images.githubusercontent.com/33947539/155710066-832551d2-b366-495b-9938-6cfe8fe703a5.png)

**Explanation:**

This is a good use case for S3 Standard-IA, which provides immediate access and 99.9% availability.

INCORRECT: “Transition objects from Amazon S3 Standard to the GLACIER storage class.” is incorrect. The Glacier storage class does not provide immediate access. You can retrieve it within hours or minutes, but you need to submit a job to retrieve the data.

INCORRECT: “Transition objects to expire after 5 years.” is incorrect. Expiring the objects after 5 years is going to delete them at the end of the 5-year period, but you still need to work out the best storage solution to use before then, and this answer does not provide that solution.

CORRECT: “Transition objects from Amazon S3 Standard to Amazon S3 Standard-Infrequent Access (S3 Standard-IA).” is the correct answer.

INCORRECT: “Transition objects from Amazon S3 Standard to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).” is incorrect. The S3 One Zone-IA tier provides immediate access, but the availability is lower at 99.5%, so this is not the best option.

![image](https://user-images.githubusercontent.com/33947539/155711820-f56ea7ae-c9d1-4846-952f-077b0780d14d.png)

**Explanation**: 

Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution from the options given above that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.

INCORRECT: “AWS Database Migration Service with change data capture” is incorrect as the DMS is used for data migration from a source to a destination. However, in this example, we need a multi-master database, and DMS will not allow this configuration.

CORRECT: “Amazon DynamoDB with global tables” is the correct answer.

INCORRECT: “Amazon Athena with Amazon S3 Cross-Region Replication” is incorrect. Amazon Athena with S3 Cross-Region Replication is not suitable because it does not. This is not a solution that provide a transactional database solution (Athena is used for analytics) or active-active synchronization.

INCORRECT: “Amazon Aurora Global Database” is incorrect. While Amazon Aurora Global Database provides read access to a database in multiple regions, it does not provide an active-active configuration with bi-directional synchronization (although you can failover to your read-only DBs and promote them to writable).

![image](https://user-images.githubusercontent.com/33947539/155713220-5931c4a5-036d-4061-9ea9-a991558da7d4.png)

**Explanation:**

DynamoDB Streams help you keep or provide a list of item level changes that have taken place in the last 24 hours. Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers, which are pieces of code that automatically respond to events in DynamoDB Streams.

If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table’s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.

An event source mapping identifies a poll-based event source for a Lambda function. It can be either an Amazon Kinesis or DynamoDB stream. Event sources maintain the mapping configuration except for stream-based services (e.g., DynamoDB, Kinesis), for which the configuration is made on the Lambda side, and Lambda performs the polling.

INCORRECT: “Enable server access logging, and create an event source mapping between AWS Lambda and the S3 bucket to which the logs are written.” is incorrect. The question asks for a solution that runs code in response to changes in a DynamoDB table, not an S3 bucket.

CORRECT: “Enable DynamoDB Streams, and create an event source mapping between AWS Lambda and the relevant stream.” is the correct answer.

INCORRECT: “Create a local secondary index that records item level changes, and write some custom code that responds to updates to the index.” is incorrect. A local secondary index maintains an alternate sort key for a given partition key value; it does not record item level changes.

INCORRECT: “Use Kinesis Data Streams, and configure DynamoDB as a producer.” is incorrect. You cannot configure DynamoDB as a Kinesis Data Streams producer.

![image](https://user-images.githubusercontent.com/33947539/155978689-ccac599e-5450-40aa-9560-e3b26548d58f.png)

Explanation: To add high availability to this architecture, both the web and database tiers require changes. For the web tier, an Auto Scaling group across multiple AZs with an ALB will ensure that there are always instances running and traffic is being distributed to them. The database tier should be migrated from the EC2 instances to Amazon RDS to take advantage of a managed database with Multi-AZ functionality. This will ensure that if there is an issue preventing access to the primary database, a secondary database can take over.

INCORRECT: “Create new public and private subnets in the same AZ for high availability.” is incorrect as this would not add high availability.

CORRECT: “Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs.” is the correct answer.

INCORRECT: “Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB).” is incorrect because the existing servers are in a single subnet. For HA, we need to instances in multiple subnets.

INCORRECT: “Create new public and private subnets in a new AZ. Create a database using Amazon EC2 in one AZ.” is incorrect because we also need HA for the database layer.

CORRECT: “Create new public and private subnets in the same VPC, each in a new AZ. Migrate the database to an Amazon RDS multi-AZ deployment.” is the correct answer.

