## Dynamo DB Optimistic lock
https://aws-blog.de/2021/07/implementing-optimistic-locking-in-dynamodb-with-python.html

## DynamoDB TTL

- DynamoDB Time to Live (TTL) enables a per-item timestamp to determine when an item is no longer needed.
- After the date and time of the specified timestamp, DynamoDB deletes the item from the table without consuming any write throughput.
- DynamoDB TTL is provided at no extra cost and can help reduce the data storage by retaining only required data.
- Items are deleted from the table are also removed from any local secondary index and global secondary index in the same way as a DeleteItem operation.

## DynamoDB Cross-region Replication
- DynamoDB cross-region replication allows identical copies (called replicas) of a DynamoDB table (called master table) to be maintained in one or more AWS regions
- Writes to the table will be automatically propagated to all replicas
- Cross-region replication currently supports single master mode. A single master has one master table and one or more replica tables
- Read replicas are updated asynchronously as DynamoDB acknowledges a write operation as successful once it has been accepted by the master table.
-  The write will then be propagated to each replica with a slight delay.

## DynamoDB Global Tables
- DynamoDB Global Tables is a new multi-master, cross-region replication capability of DynamoDB to support data access locality and regional fault tolerance for database workloads.
- Applications can now perform reads and writes to DynamoDB in AWS regions around the world, with changes in any region propagated to every region where a table is replicated.
- Global Tables help in building applications to advantage of data locality to reduce overall latency.
- Global Tables ensures eventual consistency.
- Global Tables replicates data among regions within a single AWS account, and currently does not support cross account access
- Global Tables requires DynamoDB streams enabled with New and Old image setting 

## DynamoDB Streams

- DynamoDB Streams provides a time-ordered sequence of item-level changes made to data in a table.
- DynamoDB Streams stores the data for last 24 hours, after which they are erased.
- DynamoDB Streams maintains ordered sequence of the events per item however sequence across item are not maintained.
- DynamoDB Streams have to be enabled on a per-table basis.
- DynamoDB Streams is designed for No Duplicates, so that every update made to the table will be represented exactly once in the stream.
- DynamoDB streams can be used for multi-region replication to keep other data stores up-to-date with the latest changes to DynamoDB or to take actions based on the changes made to the table

### What Problem it solves ?
Imagine your application inserts an entry (a user object) into a DyanmoDB table. A Lambda function is interested in processing that record to send the new user a welcome email.

**How would you implement that?**

There are two options:

ðŸ‘‰ The first application has to make a Dual Write. Meaning that it has to write to DynamoDB as well as notify Lambda about it. Perhaps, publish an event on an SNS 
    topic?

ðŸ‘‰ The Lambda has to poll the DynamoDB table periodically.

Both approaches are seemingly complicated or inefficient.

ðŸ¤” *What if the DynamoDB table streams the item-level changes being made?*

>Fortunately, DynamoDB provides a Change Data Capture (CDC) mechanism for each table.
>That means, if someone creates a new entry or modifies an item in a table, DynamoDB immediately emits an event containing the information about the change. You can build applications that consume these events and take action based on the contents.

![image](https://user-images.githubusercontent.com/33947539/163700806-c43edc37-c0b7-4474-b1e0-cd8aecbe1ff3.png)

#### Change Data Capture for DynamoDB Streams

Essentially, a DynamoDB stream is a time-ordered flow of information about item-level modifications in a DynamoDB table. When you enable a stream on a table, DynamoDB records every modification to items in the table and appends them to a log.

Every stream-enabled DynamoDB table gets a dedicated log file that keeps the change information for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified near real-time.

##### Shards

Stream records are organized into groups, or shards. Each shard acts as a container for multiple stream records and contains information required for accessing and iterating through these records. The stream records within a shard are removed automatically after 24 hours.
Shards allow multiple consumers to read stream records in parallel to increase the processing throughput. Default limits allow up to 2 simultaneous consumers per shard.

![image](https://user-images.githubusercontent.com/33947539/163700920-dfcc8fc2-11c9-4890-b8e2-95621f6dde41.png)

##### Each stream gets a unique ARN
DynamoDB creates a new stream with a unique stream descriptor when a stream is enabled on a table. If you disable and re-enable a stream on the table, a new stream is created with a different stream descriptor.

Every stream is uniquely identified by an Amazon Resource Name (ARN). The following is an example ARN for a stream on a DynamoDB table named TestTable.
arn:aws:dynamodb:us-west-2:111122223333:table/TestTable/stream/2015â€“05â€“11T21:21:33.291

##### To access a stream and process the stream records within, you must do the following:

- Find out the unique ARN of the stream.
- Determine which shards in the stream contain the stream records that you are interested in.
- Access the shards and retrieve the stream records that you want.

## DynamoDB auto scaling

DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you donâ€™t pay for unused provisioned capacity.

**How Application Auto Scaling works**
- You create a scaling policy for a table or a global secondary index.
- The scaling policy specifies whether you want to scale read capacity or write capacity (or both) and the minimum and maximum provisioned capacity unit settings for the table     or index.

## High availability approaches for databases#
- If possible, choose DynamoDB over RDS because of inherent fault tolerance.
- If DynamoDB canâ€™t be used, choose Aurora because of redundancy and automatic recovery features.
- If Aurora canâ€™t be used, choose Multi-AZ RDS.
- If the database runs on EC2, you have to design the HA yourself.
- Frequent RDS snapshots can protect against data corruption or failure, and they wonâ€™t impact the performance of Multi-AZ deployment.
- Regional replication is also an option, but it wonâ€™t be strongly consistent.
