
## Replica Configuration and Placement

with DynamoDB, Amazon makes these decisions for you. The number of replicas in DynamoDB is equal to 3 and the replicas are placed one per AZ, by default and this is not configurable. This way, irrespective of the individual capabilities of the user, DynamoDB offers a certain standard of resiliency.


![image](https://user-images.githubusercontent.com/33947539/155928662-e966e0ca-923b-4d34-ae66-7d7799543d5a.png)

## Consistency
DynamoDB has two configurable consistency levels for reads â€” eventual consistency and strong consistency. For writes, the consistency level is not configurable at all.

Writes are always strongly consistent â€” every write is synchronously written to two replicas (2 AZs) and asynchronously written to one more. This ensures that any data are written is durable for the life of the datastore. In terms of reads, if consistency is absolutely important (example: travel wallet balance) to the use case, users can choose strong consistency â€” where DynamoDB will ensure that the data read is certainly the latest. If not (example: list of landmarks in an area), eventual consistency is a better option because its cheaper and faster.
Such simple options relieves a lot of burden from the user.

## Batching
Batch operations are not atomic. In case of atomic operation if 100 writes are grouped into a single batch and one of the writes fail, then the entire batch is marked as failed â€” all or nothing.
With DynamoDB, there is no such guarantee. If 100 writes are grouped into a single batch and one of the writes fail, the batch operation returns information about the one request that failed, so that you can diagnose the problem and retry the operation on that item.

## Administration

DynamoDB is a hosted service. So, the majority of the administrative work is taken care of, by Amazon. In Amazonâ€™s own words:

>DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database, so that you donâ€™t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.

## Capacity Planning

Assume, you have a new service that is expected to read from the data store at 12000 reads/sec, with each read ~ 4 KB.

**How DynamoDB handles this?**

The answer is 4 partitions*. You need 4 partitions to run a workload that reads at 12000 reads/sec. How do I know it? Because, DynamoDB offers a simpler capacity planning model, where every partition can handle no more than 3000 RCU or 1000 WCU.

ðŸ‘‰ With DynamoDB, the maximum allowed item size is 400 KB. If you are using large blobs that are expected to exceed this limit, you might be better off looking at other alternatives. Cassandra, for example, has a maximum single column value size limit of 2 GB (<1 MB recommended).

ðŸ‘‰ With DynamoDB, every read and write is rounded off to 4 KB and 1 KB respectively. So, if the payload for your reads and writes are only a few bytes, you might be paying for so much more than what you are using



